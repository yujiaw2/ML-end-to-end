# ML Deploy Demo

This project demonstrates a minimal, end-to-end machine learning deployment system using scikit-learn, FastAPI, and Uvicorn. The service loads a pre-trained model and exposes HTTP endpoints for prediction.

## ğŸ“ Project Structure

```
ML-end-to-end/
â”œâ”€â”€ app/                # FastAPI application code
â”‚   â””â”€â”€ main.py         # API endpoints and model serving logic
â”œâ”€â”€ models/             # Trained model files (model_v1.pkl, model_v2.pkl)
â”œâ”€â”€ notebooks/          # One-off scripts and notebooks like model training
â”‚   â”œâ”€â”€ 01.train.ipynb
â”‚   â””â”€â”€ 02.train_mlflow.ipynb
â”œâ”€â”€ log/                # Log files generated by FastAPI service
â”‚   â””â”€â”€ prediction.log
â”œâ”€â”€ tests/              # test functions
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_api_mocked.py
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Dockerfile          # Container build file
â”œâ”€â”€ docker-compose.yml  # Container service configuration
â”œâ”€â”€ .env                # Environment variable file (not committed)
â”œâ”€â”€ pytest.ini          # Pytest config for Python path
â””â”€â”€ README.md           # Project documentation
```

## ğŸš€ Quick Start

### 0. Define environment variables
Create a `.env` file in the project root:
```bash
MODEL_DIR=models
LOG_DIR=log
DEFAULT_MODEL_LOCAL=model_v1.pkl
```

### 1. Set up virtual environment
```bash
python3 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 2. Train and save model
Refer to `notebooks/01.train.ipynb` and `notebooks/01.train_mlflow.ipynb` 

### 3. Run with Docker Compose
```bash
docker compose up --build
```

Visit Swagger UI at: [http://localhost:8000/docs](http://localhost:8000/docs)


## ğŸ“’ Using MLflow

This project supports both local model loading and **MLflow Model Registry**.

To use MLflow:
1. Set the following in `.env`:
   ```env
   USE_MLFLOW=true
   MLFLOW_TRACKING_URI=file:///app/mlruns
   DEFAULT_MODEL_MLFLOW=housing_price_model
   DEFAULT_MODEL_MLFLOW_ALIAS=prod

2. Train and register model (recommended: run inside Docker):
```bash
docker compose exec ml-api-demo bash
python notebooks/02.train_mlflow.py
```

3. Verify in UI:
```bash
mlflow ui --backend-store-uri mlruns
```

4. When running FastAPI, it will automatically pull the registered model based on the alias.
â— Tip: Make sure to **train and register from inside the container**, so that the artifact paths are valid.


## ğŸ” API Endpoints

### `/predict` (POST)
Predict on a single input vector of 8 features (adjust as needed).
```json
[8.3252,41,6.98412698,1.02380952,322,2.55555556,37.88,-122.23]
```
Response:
```json
{
  "input": [8.3252,41,6.98412698,1.02380952,322,2.55555556,37.88,-122.23],
  "prediction": 4.265793
}
```

### `/predict_batch` (POST)
Predict on multiple input vectors.
```json
[
  [8.3252,41,6.98412698,1.02380952,322,2.55555556,37.88,-122.23],
  [7.2574,52,8.28813559,1.07344633,496,2.80225989,37.85, -122.24]
]
```
Response:
```json
{
  "predictions": [4.265793, 3.75166]
}
```

### `/models` (GET)
Returns available model filenames under `models/`.

### `/use_model` (POST)
Switch to a different model file.
```json
{"model_name": "model_v2.pkl"}
```


## ğŸªµ Logging
All requests and predictions are logged to `log/prediction.log` with timestamps.


## ğŸ§ª Testing
### 1. Install pytest
```bash
pip install pytest
```

### 2. Run tests
```bash
pytest tests/
```

Test cases include:
- Model loading and shape validation
- FastAPI endpoints (/predict, /predict_batch, /use_model)
- Input validation and expected failure responses
- Mock-based tests for CI pipelines


## ğŸ³ Docker Notes
If you're using Docker directly:
```dockerfile
CMD ["uvicorn", "app.main:create_app", "--factory", "--host", "0.0.0.0", "--port", "8000"]
```
Note: Factory loading is required when using FastAPI's lifespan startup.


## ğŸ§± Tech Stack
- Python 3.10+
- scikit-learn
- FastAPI
- Uvicorn
- joblib
- numpy
- python-dotenv
- Docker
- Docker Compose
- Pytest
- GitHub Actions
- MLflow

## ğŸ“Œ Future Extensions
- .env multi-environment support (.env.dev, .env.prod)
- MLflow remote tracking server
- Streamlit or React frontend
- Gunicorn + Nginx production deployment

## ğŸ› ï¸ Development Log
### 06/15/2025 Updates
- âœ… Created model training pipeline
- âœ… Set up Fast API + Uvicorn local service, supporting single and batch prediction
- âœ… Added exception handling and logging

### 06/22/2025 Updates
- âœ… Completed containerization with docker and docker-compose
- âœ… Added basic unit tests for model prediction
- âœ… Set up volume mount for model and log files

### 06/29/2025 Updates
- âœ… Added comprehensive API tests using `fastapi.testclient`
- âœ… Updated unit tests for model loading, prediction, and failure modes
- âœ… Configured dynamic `BASE_DIR` + `.env` variable for path management
- âœ… Enabled dynamic model switching & model directory scanning
- âœ… Introduced `Pydantic`-based input validation with `field_validator`
- âœ… Fixed module resolution for pytest via `pytest.ini` and `sys.path` patching
- âœ… All tests passing; preparing for CI/CD and MLflow integration

### 06/30/2025 Updates
- âœ… Refactored to use `create_app()` with lifespan for safe model loading.
- âœ… Added mock-based tests (`test_api_mocked.py`) to support CI without real model files.
- âœ… Integrated GitHub Actions for CI with separate test workflows.
- âœ… Fixed test failures by aligning TestClient with async lifespan.
- âœ… Updated Docker config to support FastAPI factory (--factory flag).

### 07/06/2025 Updates
- âœ… Set up local MLflow tracking server, monitor the training process, record the parameters and metrics, and register models
- âœ… Got familiar with experiments, runs, model register, model tag with MLflow
- âœ… Updated main.py to support model loading from either MLflow registry or local files, use environment variable to 

#### ğŸ“Œ Reflection & Notes
- â— Avoid registering models on the host if the container will be loading them, since artifact_location will be saved as an absolute host path and cannot be resolved inside the container
- âœ… The correct approach is to train and register models inside the container, or use a shared remote backend
- âœ… With FastAPI + MLflow integration, model loading logic should be handled inside a lifespan function to ensure proper startup behavior


## ğŸ“š Resources

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [FastAPI Lifespan Docs](https://fastapi.tiangolo.com/advanced/events/)
- [Docker Compose Reference](https://docs.docker.com/compose/)


###
---

Feel free to fork, adapt, and build on this template!

